{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5b4eb5",
   "metadata": {},
   "source": [
    "# Neuron Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85e0da",
   "metadata": {},
   "source": [
    "This notebook contains all functions used for scoring and pruning neurons from a LLM using WandA when prompting the model with black and white prompts. This includes all model versions (with/without responses and specific/common pruning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61fe613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43881bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to use specified GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e741b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model ID\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604870d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716f62d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbc6c8a1f714eb59e70f65dda259d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model with specified parameters\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    cache_dir='llm_weights', \n",
    "    low_cpu_mem_usage=True, \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "218b52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompts(prompts, tokenizer, model, responses=None):\n",
    "    \"\"\"\n",
    "    Tokenizes prompts with or without responses using a tokenizer and model.\n",
    "\n",
    "    Args:\n",
    "        prompts (list): A list of prompts.\n",
    "        tokenizer: The tokenizer to apply chat templates.\n",
    "        model: The model to set the device for tensor processing.\n",
    "        responses (list, optional): A list of responses corresponding to the prompts. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokenized prompts (and responses if provided).\n",
    "    \"\"\"\n",
    "    tokenized_prompts = []\n",
    "\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        if responses:\n",
    "            # If responses are provided, include them in the messages\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": responses[i]}\n",
    "            ]\n",
    "        else:\n",
    "            # If responses are not provided, only use the prompt\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "\n",
    "        input_tensor = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True\n",
    "        ).to(model.device)\n",
    "        \n",
    "        tokenized_prompts.append(input_tensor)\n",
    "    \n",
    "    return tokenized_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c4cc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_calibration_input(model, tokenized_prompts, device):\n",
    "    \"\"\"\n",
    "    Prepares input tensors for calibration by intercepting the inputs to the first layer of the model.\n",
    "\n",
    "    Args:\n",
    "        model: The pre-trained model whose inputs need to be prepared for calibration.\n",
    "        tokenized_prompts (list): A list of tokenized prompts, each containing input tensors like 'input_ids' and 'attention_mask'.\n",
    "        device (torch.device): The device (CPU or GPU) to which the model and inputs are to be moved.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - inps (list): A list of input tensors captured from the first layer.\n",
    "            - outs (list): A list of None values, initialized for potential outputs.\n",
    "            - attention_mask (list): A list of attention masks captured from the first layer.\n",
    "            - position_ids (list): A list of position IDs captured from the first layer.\n",
    "    \"\"\"\n",
    "    # Disable cache for model configuration to avoid using cached states\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    # Retrieve the model layers\n",
    "    layers = model.model.layers\n",
    "\n",
    "    # Adjust the device if the 'model.embed_tokens' layer is mapped to a different device\n",
    "    if \"model.embed_tokens\" in model.hf_device_map:\n",
    "        device = model.hf_device_map[\"model.embed_tokens\"]\n",
    "        \n",
    "    # Lists to store inputs, attention masks, and position IDs intercepted from the first layer\n",
    "    inps = []\n",
    "    attention_mask = []\n",
    "    position_ids = []\n",
    "\n",
    "    # A custom module to intercept inputs at the first layer\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps.append(inp)\n",
    "            attention_mask.append(kwargs[\"attention_mask\"])\n",
    "            position_ids.append(kwargs[\"position_ids\"])\n",
    "            # Raise an error to stop forward pass after capturing inputs\n",
    "            raise ValueError\n",
    "\n",
    "    # Replace the first layer of the model with the Catcher module to capture inputs\n",
    "    layers[0] = Catcher(layers[0])\n",
    "        \n",
    "    # Process each batch of tokenized prompts\n",
    "    for batch in tokenized_prompts:\n",
    "        try:\n",
    "            # Forward pass to capture inputs; no need for outputs\n",
    "            model(input_ids=batch['input_ids'].to(device), \n",
    "                  attention_mask=batch['attention_mask'].to(device), \n",
    "                  position_ids=None)\n",
    "            # Clear CUDA cache to manage memory usage\n",
    "            torch.cuda.empty_cache()\n",
    "        except ValueError:\n",
    "            # Catch the ValueError to continue processing after input capture\n",
    "            pass\n",
    "        \n",
    "    # Restore the original first layer of the model\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    # Prepare an empty list for potential outputs (currently not used)\n",
    "    outs = [None for _ in range(len(tokenized_prompts))]\n",
    "    \n",
    "    # Restore the original cache setting for the model\n",
    "    model.config.use_cache = use_cache\n",
    "\n",
    "    return inps, outs, attention_mask, position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60274282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_layers(module, layers=[nn.Linear], name=\"\"):\n",
    "    \"\"\"\n",
    "    Recursively finds layers of a specified type(s) within a PyTorch module.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): The PyTorch module to search within.\n",
    "        layers (list): A list of layer types (e.g., [nn.Linear, nn.Conv2d]) to find.\n",
    "        name (str): The base name for the module, used to construct full layer names.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are the names of the layers and values are the layer objects.\n",
    "    \"\"\"\n",
    "    # Base case: If the current module's type is in the list of layers to find, return it\n",
    "    if type(module) in layers:\n",
    "        return {name: module}\n",
    "    \n",
    "    # Recursive case: Initialize an empty dictionary to store found layers\n",
    "    res = {}\n",
    "    \n",
    "    # Iterate over all child modules\n",
    "    for child_name, child_module in module.named_children():\n",
    "        # Construct the full name for the child module\n",
    "        full_name = name + \".\" + child_name if name != \"\" else child_name\n",
    "        \n",
    "        # Recursively search for layers in the child module and update the results dictionary\n",
    "        res.update(find_layers(child_module, layers=layers, name=full_name))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "302bb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedGPT:\n",
    "    \"\"\"\n",
    "    This class wraps a GPT layer to perform specific operations, such as tracking\n",
    "    activations, scaling inputs, and managing device placement.\n",
    "\n",
    "    Attributes:\n",
    "        layer (nn.Module): The GPT layer to be wrapped.\n",
    "        dev (torch.device): The device where the layer's weight is stored.\n",
    "        rows (int): The number of rows in the layer's weight matrix.\n",
    "        columns (int): The number of columns in the layer's weight matrix.\n",
    "        scaler_row (torch.Tensor): A tensor used to scale the input activations.\n",
    "        activations (list): A list to store activations (currently unused).\n",
    "        nsamples (int): The number of samples processed.\n",
    "        layer_id (int): Identifier for the layer.\n",
    "        layer_name (str): Name of the layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer, layer_id=0, layer_name=\"none\"):\n",
    "        \"\"\"\n",
    "        Initializes the WrappedGPT object with the provided layer and metadata.\n",
    "\n",
    "        Args:\n",
    "            layer (nn.Module): The GPT layer to be wrapped.\n",
    "            layer_id (int): An identifier for the layer (default is 0).\n",
    "            layer_name (str): A name for the layer (default is \"none\").\n",
    "        \"\"\"\n",
    "        self.layer = layer\n",
    "        self.dev = self.layer.weight.device\n",
    "        self.rows = layer.weight.data.shape[0]\n",
    "        self.columns = layer.weight.data.shape[1]\n",
    "\n",
    "        # Initialize the scaler tensor and other attributes\n",
    "        self.scaler_row = torch.zeros((self.columns), device=self.dev)\n",
    "        self.activations = []  # Currently unused\n",
    "        self.nsamples = 0\n",
    "\n",
    "        # Metadata for the layer\n",
    "        self.layer_id = layer_id\n",
    "        self.layer_name = layer_name\n",
    "\n",
    "    def add_batch(self, inp, out):\n",
    "        \"\"\"\n",
    "        Processes a batch of input and output data, updating the scaler and managing activations.\n",
    "\n",
    "        Args:\n",
    "            inp (torch.Tensor): Input tensor of shape (batch_size, seq_len) or (batch_size, seq_len, hidden_dim).\n",
    "            out (torch.Tensor): Output tensor of shape (batch_size, seq_len, hidden_dim).\n",
    "        \"\"\"\n",
    "        # Ensure input and output tensors have three dimensions\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        if len(out.shape) == 2:\n",
    "            out = out.unsqueeze(0)\n",
    "\n",
    "        batch_size = inp.shape[0]\n",
    "\n",
    "        # Check if the layer is a Linear layer and reshape input if necessary\n",
    "        if isinstance(self.layer, nn.Linear):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "            inp = inp.t()\n",
    "\n",
    "        # Update the scaler for the rows based on the number of samples processed\n",
    "        self.scaler_row *= self.nsamples / (self.nsamples + batch_size)\n",
    "        self.nsamples += batch_size\n",
    "\n",
    "        # Convert input to float32 for numerical stability\n",
    "        inp = inp.type(torch.float32)\n",
    "\n",
    "        # Update scaler_row with the squared L2 norm of the input\n",
    "        self.scaler_row += torch.norm(inp, p=2, dim=1) ** 2 / self.nsamples\n",
    "\n",
    "        # Optionally store activations (currently disabled to save memory)\n",
    "        # self.activations.append(inp)\n",
    "\n",
    "        # Clean up to manage memory\n",
    "        del inp\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "322d897e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_model_layers(df, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Processes model layers by tokenizing prompts, preparing calibration inputs,\n",
    "    and calculating metrics for each layer based on input activations.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing columns 'variation', 'race', 'prompt_text', and 'response'.\n",
    "        model (nn.Module): The pre-trained model to process.\n",
    "        tokenizer: The tokenizer to use for encoding prompts and responses.\n",
    "    \"\"\"\n",
    "    # Iterate through each group of data by variation and race\n",
    "    for i, group in df.groupby([\"variation\", \"race\"]):\n",
    "        # Disable caching for model configuration to avoid using cached states\n",
    "        use_cache = model.config.use_cache\n",
    "        model.config.use_cache = False\n",
    "        \n",
    "        # Extract variation and race\n",
    "        variation = group.variation.iloc[0]\n",
    "        race = group.race.iloc[0]\n",
    "        \n",
    "        # Tokenize prompts and responses\n",
    "        inps_enc = tokenize_prompts(\n",
    "            group.prompt_text.tolist(), \n",
    "            tokenizer, \n",
    "            model,\n",
    "            group.response.tolist()\n",
    "        )\n",
    "        \n",
    "        print(f\"Starting for {variation} variation and race {race} with responses\")\n",
    "        print(\"Tokenization complete. Prompts are ready for further processing.\")\n",
    "        print(f\"We have {len(inps_enc)} prompts.\")\n",
    "        \n",
    "        # Prepare calibration inputs by passing tokenized prompts through the embedding layer\n",
    "        with torch.no_grad():\n",
    "            inps, outs, attention_mask, position_ids = prepare_calibration_input(model, inps_enc, model.device)\n",
    "        \n",
    "        print(\"Calibration input prepared. Ready for scoring.\")\n",
    "\n",
    "        # Move input tensors to the correct device\n",
    "        inps = [inp.squeeze(0).to(model.device) for inp in inps]\n",
    "        attention_mask = [am.to(model.device) for am in attention_mask]\n",
    "        position_ids = [pids.to(model.device) for pids in position_ids]\n",
    "        layers = model.model.layers\n",
    "\n",
    "        print(\"Inputs, attention masks, and position IDs are prepared and moved to the correct device.\")\n",
    "\n",
    "        # Process each layer in the model\n",
    "        for i in range(len(layers)):\n",
    "            layer = layers[i]\n",
    "            subset = find_layers(layer)  # Find all relevant sublayers\n",
    "            \n",
    "            if f\"model.layers.{i}\" in model.hf_device_map:\n",
    "                # Handle multi-GPU cases\n",
    "                dev = model.hf_device_map[f\"model.layers.{i}\"]\n",
    "                inps = [inp.to(dev) for inp in inps]\n",
    "                outs = [out.to(dev) if out is not None else out for out in outs]\n",
    "                attention_mask = [am.to(dev) for am in attention_mask]\n",
    "                position_ids = [pids.to(dev) for pids in position_ids]\n",
    "\n",
    "            # Wrap layers with WrappedGPT for activation tracking\n",
    "            wrapped_layers = {name: WrappedGPT(subset[name]) for name in subset}\n",
    "\n",
    "            def add_batch(name):\n",
    "                \"\"\"Helper function to add a batch for each wrapped layer.\"\"\"\n",
    "                def tmp(_, inp, out):\n",
    "                    wrapped_layers[name].add_batch(inp[0].data, out.data)\n",
    "                return tmp\n",
    "\n",
    "            # Process inputs through the current layer and register forward hooks\n",
    "            for j in range(len(inps)):\n",
    "                handles = []\n",
    "                for name in wrapped_layers:\n",
    "                    handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outs[j] = layer(\n",
    "                        inps[j].unsqueeze(0),\n",
    "                        attention_mask=attention_mask[j],\n",
    "                        position_ids=position_ids[j],\n",
    "                    )[0]\n",
    "\n",
    "                for h in handles:\n",
    "                    h.remove()\n",
    "\n",
    "            # Calculate and save scores for each sublayer\n",
    "            for name in subset:\n",
    "                print(f\"Scoring layer {i} name {name}\")\n",
    "\n",
    "                magnitude = torch.abs(subset[name].weight.data)\n",
    "                act = torch.sqrt(wrapped_layers[name].scaler_row.reshape((1, -1)))\n",
    "                W_metric = magnitude * act\n",
    "\n",
    "                # Save the calculated scores\n",
    "                save_folder = os.path.join(f\"scores_all/wanda_scores_{variation}_with_responses/{race}_weights_{variation}_with_responses\")\n",
    "                os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "                target_file = os.path.join(\n",
    "                    save_folder,\n",
    "                    f\"W_metric_layer_{i}_name_{name}_{race}_weights_{variation}_with_responses.pkl\",\n",
    "                )\n",
    "\n",
    "                with open(target_file, \"wb\") as f:\n",
    "                    print(\n",
    "                        f\"Writing W_metric in layer {i} and name {name} with {race} prompts, {variation} variation with responses to the file\"\n",
    "                    )\n",
    "                    pickle.dump(W_metric, f)\n",
    "        \n",
    "        # Swap inps and outs for further processing\n",
    "        for j in range(len(inps)):\n",
    "            with torch.no_grad():\n",
    "                outs[j] = layer(\n",
    "                    inps[j].unsqueeze(0),\n",
    "                    attention_mask=attention_mask[j],\n",
    "                    position_ids=position_ids[j],\n",
    "                )[0].squeeze(0)\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "        # Restore original cache setting and clear CUDA cache\n",
    "        model.config.use_cache = use_cache\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c42a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_layers = {\n",
    "    'q' : 'self_attn.q_proj',\n",
    "    'k' : 'self_attn.k_proj',\n",
    "    'v' : 'self_attn.v_proj',\n",
    "    'o' : 'self_attn.o_proj',\n",
    "    'gate' : 'mlp.gate_proj',\n",
    "    'up' : 'mlp.up_proj',\n",
    "    'down' : 'mlp.down_proj'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cba9912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_from_training_common_neurons(model, tokenizer, csv_file):\n",
    "    \"\"\"\n",
    "    Prunes neurons from a model's layers based on training data of common neurons across a training set of prompts provided in a CSV file.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The pre-trained model whose neurons are to be pruned.\n",
    "        tokenizer: The tokenizer associated with the model (not used directly in this function but included for consistency).\n",
    "        csv_file (str): Path to the CSV file containing columns: 'layer_num', 'sublayer', 'neuron_index', and 'num_prompts'.\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Module): The pruned model.\n",
    "        tokenizer: The unchanged tokenizer.\n",
    "    \"\"\"\n",
    "    # Save the model's original cache configuration and disable caching to prevent using cached states\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "\n",
    "    # Read and preprocess the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df[df[\"num_prompts\"] == 12].copy()  # Filter rows where 'num_prompts' equals 12, that is those neurons which appear in every training prompt\n",
    "\n",
    "    # Ensure 'layer_num' and 'neuron_index' are treated as integers\n",
    "    df['layer_num'] = df['layer_num'].astype(int)\n",
    "    df[\"neuron_index\"] = df[\"neuron_index\"].apply(lambda x: int(x.split('.')[2]))\n",
    "\n",
    "    # Group the DataFrame by 'layer_num' and 'sublayer' to process each combination\n",
    "    grouped = df.groupby(['layer_num', 'sublayer'])\n",
    "\n",
    "    # Process each group of layers and sublayers\n",
    "    for (layer_num, name_layer_alias), group in grouped:\n",
    "        # Retrieve the specific layer and its subset for pruning\n",
    "        layer = layers[layer_num]\n",
    "        subset = find_layers(layer)\n",
    "        \n",
    "        # Map the alias to the actual name of the sublayer\n",
    "        name_layer = map_layers[name_layer_alias]\n",
    "        \n",
    "        # Skip if the sublayer is not in the subset\n",
    "        if name_layer not in subset:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing layer {layer_num}, sublayer {name_layer}\")\n",
    "\n",
    "        # Get the neuron indices to prune for this specific layer and sublayer\n",
    "        prune_indices = group['neuron_index'].values\n",
    "        \n",
    "        # Calculate the rows and columns for the indices to prune\n",
    "        weight_dim = subset[name_layer].weight.data.shape[1]\n",
    "        prune_rows = prune_indices // weight_dim\n",
    "        prune_cols = prune_indices % weight_dim\n",
    "\n",
    "        # Create a mask to set the weights of the pruned neurons to zero\n",
    "        W_mask = torch.zeros_like(subset[name_layer].weight.data) == 1\n",
    "        W_mask[prune_rows, prune_cols] = True\n",
    "        subset[name_layer].weight.data[W_mask] = 0  # Prune weights by setting them to zero\n",
    "\n",
    "        # Calculate and print the percentage of pruned weights\n",
    "        total_weights = subset[name_layer].weight.data.numel()\n",
    "        num_pruned_weights = len(prune_indices)\n",
    "        prune_ratio = num_pruned_weights / total_weights\n",
    "        print(f\"Layer {layer_num} name {name_layer}: Pruned {num_pruned_weights} weights ({prune_ratio:.4%} of total weights)\")\n",
    "\n",
    "    # Restore the model's original cache configuration\n",
    "    model.config.use_cache = use_cache\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "231ac9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_setDiff(model, tokenizer, item, model_version, top_white_percent=0.15, top_black_percent=0.15):\n",
    "    \"\"\"\n",
    "    Prunes neurons from a model's layers based on the set difference between the top neurons\n",
    "    identified from two different groups (e.g., 'white' and 'black') for a given item (variation).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The pre-trained model whose neurons are to be pruned.\n",
    "        tokenizer: The tokenizer associated with the model (not used directly in this function but included for consistency).\n",
    "        item (str): The specific item/variation (e.g., 'chair') for which the pruning is performed.\n",
    "        model_version (str): Either 'black' or 'white' to define from which groups are the top neurons.\n",
    "        top_white_percent (float): The percentage of top neurons to select from the 'white' group.\n",
    "        top_black_percent (float): The percentage of top neurons to select from the 'black' group.\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Module): The pruned model.\n",
    "        tokenizer: The unchanged tokenizer.\n",
    "    \"\"\"\n",
    "    # Save the model's original cache configuration and disable caching to prevent using cached states\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "\n",
    "    print(f\"Pruning based on top {top_white_percent * 100}% white and top {top_black_percent * 100}% black neurons for {item} with responses.\")\n",
    "\n",
    "    # Iterate through each layer of the model\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        subset = find_layers(layer)  # Get all relevant sublayers\n",
    "        \n",
    "        # Process each sublayer within the current layer\n",
    "        for name in subset:\n",
    "            print(f\"Processing layer {i} name {name}\")\n",
    "            \n",
    "            # Load the W_metric scores for both 'white' and 'black' categories\n",
    "            W_metric_white = pickle.load(\n",
    "                open(f\"scores_all/wanda_scores_{item}_with_responses/white_weights_{item}_with_responses/W_metric_layer_{i}_name_{name}_white_weights_{item}_with_responses.pkl\", \"rb\")\n",
    "            )\n",
    "            W_metric_black = pickle.load(\n",
    "                open(f\"scores_all/wanda_scores_{item}_with_responses/black_weights_{item}_with_responses/W_metric_layer_{i}_name_{name}_black_weights_{item}_with_responses.pkl\", \"rb\")\n",
    "            )\n",
    "\n",
    "            # Ensure the tensors are moved to CPU before converting to NumPy for processing\n",
    "            W_metric_white_cpu = W_metric_white.cpu().numpy()\n",
    "            W_metric_black_cpu = W_metric_black.cpu().numpy()\n",
    "\n",
    "            # Flatten the arrays to work with them easily\n",
    "            W_metric_white_flat = W_metric_white_cpu.flatten()\n",
    "            W_metric_black_flat = W_metric_black_cpu.flatten()\n",
    "\n",
    "            # Select top % of 'white' neurons\n",
    "            num_top_white = int(top_white_percent * W_metric_white_flat.size)\n",
    "            top_white_indices = torch.topk(torch.tensor(W_metric_white_flat), num_top_white, largest=True)[1].numpy()\n",
    "\n",
    "            # Select top % of 'black' neurons\n",
    "            num_top_black = int(top_black_percent * W_metric_black_flat.size)\n",
    "            top_black_indices = torch.topk(torch.tensor(W_metric_black_flat), num_top_black, largest=True)[1].numpy()\n",
    "\n",
    "            # Find the set difference between the top 'white' and 'black' neurons\n",
    "            if model_version == \"black\":\n",
    "                prune_indices = np.setdiff1d(top_black_indices, top_white_indices)\n",
    "            else:\n",
    "                prune_indices = np.setdiff1d(top_white_indices, top_black_indices)\n",
    "\n",
    "            # Determine the rows and columns of the indices to prune\n",
    "            weight_dim = subset[name].weight.data.shape[1]\n",
    "            prune_rows = prune_indices // weight_dim\n",
    "            prune_cols = prune_indices % weight_dim\n",
    "\n",
    "            # Create a mask to set the weights of the pruned neurons to zero\n",
    "            W_mask = torch.zeros_like(subset[name].weight.data) == 1\n",
    "            W_mask[prune_rows, prune_cols] = True\n",
    "            subset[name].weight.data[W_mask] = 0  # Prune weights by setting them to zero\n",
    "\n",
    "            # Calculate and print the percentage of pruned weights\n",
    "            total_weights = subset[name].weight.data.numel()\n",
    "            num_pruned_weights = len(prune_indices)\n",
    "            prune_ratio = num_pruned_weights / total_weights\n",
    "            print(f\"Layer {i} name {name}: Pruned {num_pruned_weights} weights ({prune_ratio:.4%} of total weights) with responses\")\n",
    "\n",
    "    # Restore the model's original cache configuration\n",
    "    model.config.use_cache = use_cache\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0374bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(prompt, terminators, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generates tokenized inputs for a given prompt using a specified tokenizer and model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt text to be tokenized.\n",
    "        terminators (list): List of terminators for the prompt (not used in this function, but included for potential future use).\n",
    "        tokenizer: The tokenizer to apply the chat template for tokenization.\n",
    "        model: The model to determine the device where the inputs should be moved.\n",
    "\n",
    "    Returns:\n",
    "        inputs (torch.Tensor): Tokenized inputs ready for model processing.\n",
    "    \"\"\"\n",
    "    # Create a list of messages for the tokenizer's chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    # Apply the tokenizer's chat template to convert messages to tokenized input tensors\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)  # Move the input tensors to the model's device\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f0ca753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(df, pruned_model, pruned_tokenizer, terminators, num_iterations=100, temperature=0.6):\n",
    "    \"\"\"\n",
    "    Generates responses for a DataFrame of prompts using a pruned model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing prompt data with necessary columns.\n",
    "        pruned_model (nn.Module): The pruned model used for generating responses.\n",
    "        pruned_tokenizer: The tokenizer associated with the pruned model.\n",
    "        terminators (list): List of terminator tokens for the generation process.\n",
    "        num_iterations (int): Number of iterations to repeat the generation process. Default is 100.\n",
    "        temperature (float): The temperature parameter for sampling during generation. Default is 0.6.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing the scenario, variation, name group, name, context level, prompt text, and generated response.\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "\n",
    "    # Repeat the process for the specified number of iterations\n",
    "    for _ in tqdm(range(num_iterations), desc=\"Iterations\"):\n",
    "        # Iterate over each row in the DataFrame\n",
    "        for i, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Generating Responses\"):\n",
    "            prompt = row['prompt_text']  # Extract the prompt text from the current row\n",
    "\n",
    "            # Generate tokenized inputs using the provided tokenizer\n",
    "            input_ids = get_inputs(prompt, terminators, pruned_tokenizer, pruned_model)\n",
    "\n",
    "            # Generate output using the pruned model\n",
    "            output = pruned_model.generate(\n",
    "                input_ids,\n",
    "                eos_token_id=terminators,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "\n",
    "            # Decode the generated output after the prompt\n",
    "            response = output[0][input_ids.shape[-1]:]\n",
    "            decoded = pruned_tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "            # Prepare the output dictionary for the current prompt and response\n",
    "            output_dict = {\n",
    "                \"scenario\": row[\"scenario\"],\n",
    "                \"variation\": row[\"variation\"],\n",
    "                \"name_group\": row[\"name_group\"],\n",
    "                \"name\": row[\"name\"],\n",
    "                \"context_level\": row[\"context_level\"],\n",
    "                \"prompt_text\": row[\"prompt_text\"],\n",
    "                \"response\": decoded\n",
    "            }\n",
    "\n",
    "            # Append the output dictionary to the outputs list\n",
    "            outputs.append(output_dict)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f5f210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_top_neurons_setDiff(model, variations, model_version, folder=\"scores_all\", top_p_percent=0.15):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame-like structure containing the top neurons' indices for each layer's sublayer,\n",
    "    based on the set difference between the top neurons identified for two groups ('white' and 'black').\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The pre-trained model to process.\n",
    "        variations (list): List of variations/items to process.\n",
    "        model_version (str): Either 'black' or 'white' to define from which groups are the top neurons.\n",
    "        folder (str): Directory containing the scores files. Default is \"scores_all\".\n",
    "        top_p_percent (float): The percentage of top neurons to select. Default is 0.15 (15%).\n",
    "\n",
    "    \"\"\"\n",
    "    # Disable caching for model configuration to avoid using cached states\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "\n",
    "    # Iterate through each variation in the provided list\n",
    "    for variation in tqdm(variations, desc=\"Processing items\"):\n",
    "        rows = []  # List to store data for the DataFrame\n",
    "\n",
    "        # Iterate through each layer in the model\n",
    "        for i in range(len(layers)):\n",
    "            layer = layers[i]\n",
    "            subset = find_layers(layer)  # Find all relevant sublayers\n",
    "\n",
    "            # Process each sublayer within the current layer\n",
    "            for name in subset:\n",
    "                print(f\"Processing variation {variation}, both races, layer {i}, sublayer {name} with responses\")\n",
    "\n",
    "                # Paths to the 'white' and 'black' weights metrics\n",
    "                path_white = f\"W_metric_layer_{i}_name_{name}_white_weights_{variation}_with_responses.pkl\"\n",
    "                path_black = f\"W_metric_layer_{i}_name_{name}_black_weights_{variation}_with_responses.pkl\"\n",
    "\n",
    "                # Load the metric scores for both 'white' and 'black' categories\n",
    "                W_metric_white = pickle.load(\n",
    "                    open(f\"{folder}/wanda_scores_{variation}_with_responses/white_weights_{variation}_with_responses/{path_white}\", \"rb\")\n",
    "                )\n",
    "                W_metric_black = pickle.load(\n",
    "                    open(f\"{folder}/wanda_scores_{variation}_with_responses/black_weights_{variation}_with_responses/{path_black}\", \"rb\")\n",
    "                )\n",
    "\n",
    "                # Ensure tensors are moved to CPU before converting to NumPy\n",
    "                W_metric_white_cpu = W_metric_white.cpu().numpy()\n",
    "                W_metric_black_cpu = W_metric_black.cpu().numpy()\n",
    "\n",
    "                # Flatten the arrays to work with them easily\n",
    "                W_metric_white_flat = W_metric_white_cpu.flatten()\n",
    "                W_metric_black_flat = W_metric_black_cpu.flatten()\n",
    "\n",
    "                # Select top % of 'white' neurons\n",
    "                num_top_white = int(top_p_percent * W_metric_white_flat.size)\n",
    "                top_white_indices = torch.topk(torch.tensor(W_metric_white_flat), num_top_white, largest=True)[1].numpy()\n",
    "\n",
    "                # Select top % of 'black' neurons\n",
    "                num_top_black = int(top_p_percent * W_metric_black_flat.size)\n",
    "                top_black_indices = torch.topk(torch.tensor(W_metric_black_flat), num_top_black, largest=True)[1].numpy()\n",
    "\n",
    "                # Find the set difference between the top 'black' and 'white' neurons\n",
    "                if model_version == \"black\":\n",
    "                    prune_indices = np.setdiff1d(top_black_indices, top_white_indices)\n",
    "                else:\n",
    "                    prune_indices = np.setdiff1d(top_white_indices, top_black_indices)\n",
    "\n",
    "                # Add the top neurons to the list\n",
    "                for idx in prune_indices:\n",
    "                    df_row = {\n",
    "                        \"layer\": i,\n",
    "                        \"sublayer\": name,\n",
    "                        \"neuron_index\": idx\n",
    "                    }\n",
    "                    rows.append(df_row)\n",
    "\n",
    "        # Save the results to a pickle file after processing each variation\n",
    "        save_path = f\"{variation}_pruned_{model_version}_{int(top_p_percent * 100)}_with_responses.pkl\"\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(rows, f)\n",
    "    \n",
    "    # Restore the model's original cache configuration\n",
    "    model.config.use_cache = use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5737ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_llm",
   "language": "python",
   "name": "prune_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
