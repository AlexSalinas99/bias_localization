{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a1a589",
   "metadata": {},
   "source": [
    "This notebook runs the functions to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d671b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuron_pruning_main import *\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b8c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to use specified GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c19fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model ID\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model with specified parameters\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    cache_dir='llm_weights', \n",
    "    low_cpu_mem_usage=True, \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49bfee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\n",
    "    \"bicycle\", \"car\", \"bike\", \"motorcycle\", \"laptop\", \"sofa\", \"painting\", \"guitar\", \"refrigerator\", \"house\", \"book\", \"boat\", \"violin\", \"banjo\",\n",
    "    \"necklace\", \"chair\", \"cellphone\", \"gold_chain\"\n",
    "    ]\n",
    "training_items = ['violin',\n",
    " 'banjo',\n",
    " 'bicycle',\n",
    " 'sofa',\n",
    " 'painting',\n",
    " 'motorcycle',\n",
    " 'laptop',\n",
    " 'necklace',\n",
    " 'gold_chain',\n",
    " 'book',\n",
    " 'house',\n",
    " 'refrigerator']\n",
    "test_items = list(set(items) - set(training_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2afa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('purchase_low_all_items.csv')\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = []\n",
    "\n",
    "model_version = ['black', 'white']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f439df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_model_layers(df, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For specific models (pruning within each variation, not using common neurons)\n",
    "for version in model_version:\n",
    "    for item in items:\n",
    "        pruned_model, pruned_tokenizer = prune_setDiff(model, tokenizer, item, version, top_white_percent=0.15, top_black_percent=0.15)\n",
    "        outputs = generate_responses(df, pruned_model, pruned_tokenizer, terminators, num_iterations=100, temperature=0.6)\n",
    "        df_results = pd.DataFrame(outputs)\n",
    "        df_results.to_csv(f'full_prompts_purchase_results_pruning/setDiff_15_top_{version}_{item}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8719b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For common model\n",
    "for version in model_version:\n",
    "    create_df_top_neurons_setDiff(model, variations, version, folder=\"scores_all\", top_p_percent=0.15)\n",
    "\n",
    "# files created in previous function\n",
    "neuron_files = [f\"{i}_pruned_{r}_15.pkl\" for i, r in product(training_items, model_version)]\n",
    "\n",
    "_, _ = compute_similar_neurons(neuron_files)\n",
    "\n",
    "common_neuron_files = ['top15_w_neurons_12_items_training.csv',\n",
    "         'top15_b_neurons_12_items_training.csv']\n",
    "\n",
    "for common_neurons_file in common_neurons_files:\n",
    "    pruned_common_model, pruned_common_tokenizer = prune_from_training_common_neurons(model, tokenizer, common_neurons_file)\n",
    "    outputs_common = generate_responses(df, pruned_common_model, pruned_common_tokenizer, terminators, num_iterations=100, temperature=0.6)\n",
    "    df_results_common = pd.DataFrame(outputs_common)\n",
    "    df_results_common.to_csv(f'full_prompts_purchase_results_pruning/setDiff_15_top_{version}_{item}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_llm",
   "language": "python",
   "name": "prune_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
